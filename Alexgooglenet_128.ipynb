{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AqsaS12/Heart-disease-Prediction/blob/main/Alexgooglenet_128.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHCS3AXhgW-1"
      },
      "source": [
        "# Lib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ4z93ijKnHA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "dat = pd.read_csv('/content/drive/MyDrive/heart_2020_cleaned.csv')\n",
        "dat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVXfmmS6Eyra"
      },
      "source": [
        "# 30% of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhnlaUxaqOGU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a DataFrame named 'data' with a 'target' column containing 0s and 1s.\n",
        "\n",
        "# Calculate the number of samples to keep from each class.\n",
        "class_A_count = int(len(dat[dat['HeartDisease'] == 'Yes']) * 0.3)  # Keep 30% of class A\n",
        "class_B_count = int(len(dat[dat['HeartDisease'] == 'No']) * 0.3)  # Keep 30% of class B\n",
        "\n",
        "# Filter the data to keep 40% of each class.\n",
        "data = pd.concat([\n",
        "    dat[dat['HeartDisease'] == 'Yes'].sample(class_A_count),\n",
        "    dat[dat['HeartDisease'] =='No'].sample(class_B_count)\n",
        "])\n",
        "\n",
        "# Now, 'filtered_data' contains 40% of class A and 40% of class B.\n",
        "\n",
        "# If you want to shuffle the resulting DataFrame, you can use:\n",
        "data = data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH2h2tyFZyMe"
      },
      "outputs": [],
      "source": [
        "percentage_used = (len(data) / len(dat)) * 100\n",
        "print(f\"The percentage of data used is: {percentage_used:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-48mlszxdHk"
      },
      "source": [
        "#Data information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQKORDuCLbjT"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHtEgoamxm5N"
      },
      "source": [
        "#Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciZTPIsEMxzr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "data['HeartDisease'] = label_encoder.fit_transform(data['HeartDisease'])\n",
        "data['AlcoholDrinking'] = label_encoder.fit_transform(data['AlcoholDrinking'])\n",
        "data['Smoking'] = label_encoder.fit_transform(data['Smoking'])\n",
        "data['Stroke'] = label_encoder.fit_transform(data['Stroke'])\n",
        "data['DiffWalking'] = label_encoder.fit_transform(data['DiffWalking'])\n",
        "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
        "data['AgeCategory'] = label_encoder.fit_transform(data['AgeCategory'])\n",
        "data['Race'] = label_encoder.fit_transform(data['Race'])\n",
        "data['Diabetic'] = label_encoder.fit_transform(data['Diabetic'])\n",
        "data['PhysicalActivity'] = label_encoder.fit_transform(data['PhysicalActivity'])\n",
        "data['GenHealth'] = label_encoder.fit_transform(data['GenHealth'])\n",
        "data['Asthma'] = label_encoder.fit_transform(data['Asthma'])\n",
        "data['KidneyDisease'] = label_encoder.fit_transform(data['KidneyDisease'])\n",
        "data['SkinCancer'] = label_encoder.fit_transform(data['SkinCancer'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxmnYp4NomH3"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG7AFC-13GYw"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# # Calculate the correlation matrix\n",
        "# correlation_matrix = data.corr()\n",
        "\n",
        "# # Visualize the correlation matrix using a heatmap\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "# plt.title('Correlation Matrix')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PH0dORAcxVx"
      },
      "source": [
        "## **value counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq6j-A2kJ541"
      },
      "outputs": [],
      "source": [
        "data.HeartDisease.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Uxb50BdM86"
      },
      "source": [
        "# Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy2BveGZ5nTw"
      },
      "outputs": [],
      "source": [
        "X=data.drop(\"HeartDisease\", axis=1)\n",
        "y=data[\"HeartDisease\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUnfQyRwia53"
      },
      "source": [
        "# Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pld-uSQxQZOD"
      },
      "outputs": [],
      "source": [
        "X.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "241-M1KcsdWD"
      },
      "outputs": [],
      "source": [
        "\n",
        "X.ffill(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yhsRQmsrTS3"
      },
      "outputs": [],
      "source": [
        "X.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3h4PMuWCzTW"
      },
      "outputs": [],
      "source": [
        "float_columns = ['BMI', 'PhysicalHealth', 'SleepTime']\n",
        "\n",
        "# Converting the float columns to int with rounding\n",
        "for column in float_columns:\n",
        "    X[column] = X[column].round().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVWoWw3TXkf8"
      },
      "outputs": [],
      "source": [
        "y.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8nbSHeiU4LA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def plot_class_distribution(y):\n",
        "    \"\"\"\n",
        "    Plots a bar chart showing the distribution of classes in the target column.\n",
        "\n",
        "    Parameters:\n",
        "    y (array-like): The target column containing class labels.\n",
        "    \"\"\"\n",
        "    # Calculate the distribution of classes\n",
        "    class_counts = Counter(y)\n",
        "    labels, counts = zip(*class_counts.items())\n",
        "\n",
        "    x_pos = np.arange(len(labels))\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.bar(x_pos, counts, color=['blue', 'red'])\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xticks(x_pos, labels)\n",
        "    plt.show()\n",
        "\n",
        "# Load your dataset\n",
        "# Replace 'your_dataset.csv' with the path to your actual dataset\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# # Replace 'target_column' with the actual name of your target column\n",
        "# X = df.drop('target_column', axis=1)\n",
        "# y = df['target_column']\n",
        "\n",
        "# If you need to split into training and testing data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Plot the class distribution for the entire dataset or for y_train if split\n",
        "plot_class_distribution(y)\n",
        "# plot_class_distribution(y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4krBmp4xzVs"
      },
      "source": [
        "# Balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JnsXprx9Ja"
      },
      "source": [
        "## Smote variants install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWx_wYAsGptr"
      },
      "outputs": [],
      "source": [
        "!pip install -U smote-variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzT-NzCF0TV7"
      },
      "outputs": [],
      "source": [
        "import smote_variants as sv\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming you have your data X_ and labels y defined\n",
        "x_np=np.array(X)\n",
        "y_np=np.array(y)\n",
        "# Initialize MSMOTE oversampler\n",
        "oversampler_mysn = sv.MSMOTE(random_state=2)\n",
        "\n",
        "# Apply ProWSyn oversampling\n",
        "X_b, y_b = oversampler_mysn.sample(x_np, y_np)\n",
        "\n",
        "# Check the class distribution after oversampling\n",
        "class_counts = Counter(y_b)\n",
        "print(\"Class Distribution After Oversampling:\", class_counts)\n",
        "plot_class_distribution(y_b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1fc3SiLh1MG"
      },
      "source": [
        "# Spliting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkLhlrHBeQSa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_b,y_b,test_size=0.2)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJEjceI2CP5S"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb-0F7wHt2Kp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import  Activation, GlobalAveragePooling1D, concatenate, LayerNormalization, SpatialDropout1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from keras.layers import Dense, Dropout, GaussianNoise, Conv1D, MaxPooling1D, Flatten\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBNyRQSk-GDw"
      },
      "source": [
        "# Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyAujxdKcB2t"
      },
      "outputs": [],
      "source": [
        "#Standardizing our training and testing data.\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_train_scaledd= scaler.fit_transform(X_train)\n",
        "X_test_scaledd= scaler.transform(X_test)\n",
        "# Optional: Convert scaled data back to DataFrame for easier inspection\n",
        "X_train_scaled = pd.DataFrame(X_train_scaledd, columns=X.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaledd, columns=X.columns)\n",
        "\n",
        "print(\"Scaled Training Data:\\n\", X_train_scaled)\n",
        "print(\"Scaled Test Data:\\n\", X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvtNmGbxz30-"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSQzPdukyd25"
      },
      "source": [
        "## ALEXNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVKY8lTo1sMc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# Define the AlexNet-like model\n",
        "# # Reshape the input data for 1D convolution\n",
        "# X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "# X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "alx_model = Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(8, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
        "alx_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "alx_model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, validation_split=0.2)\n",
        "# Model evaluation on the test set\n",
        "y_pred_alx = alx_model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_alx > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "end = time.time()\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time:{exe_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbDjjXzLRa0u"
      },
      "source": [
        "## Googlenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyl4oxucdl-s"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the towers (branches)\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(32, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(input_layer)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "# Concatenate the towers\n",
        "output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "output = Flatten()(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(4, activation='relu')(output)\n",
        "output = Dropout(0.5)(output)\n",
        "# output = Dense(16, activation='tanh')(output)\n",
        "\n",
        "# Output layer for binary classification with sigmoid activation\n",
        "output_layer = Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "# Build the model\n",
        "custom_inception_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model for binary classification\n",
        "custom_inception_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "custom_inception_model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, validation_split=0.2)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_train_pred = (custom_inception_model.predict(X_train_scaled) > 0.5).astype(int)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = (custom_inception_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "# print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Googlenet Accuracy:\", test_accuracy)\n",
        "print(\"Googlenet Precision:\", precision)\n",
        "print(\"Googlenet Recall:\", recall)\n",
        "print(\"Googlenet F1 Score:\", f1)\n",
        "print(\"Googlenet Execution Time:\", execution_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h7TAFQRkGv6"
      },
      "source": [
        "## alexnet-googlnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vGP47IL2v9os"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the input layer for the AlexNet model\n",
        "alx_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "alx_model = tf.keras.Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Extract the output tensor of the AlexNet model\n",
        "alx_output = alx_model(alx_input)\n",
        "\n",
        "# Define the input layer for the Inception model\n",
        "inception_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(128, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(inception_input)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "inception_output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "inception_output = Flatten()(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(4, activation='relu')(inception_output)\n",
        "inception_output = Dropout(0.5)(inception_output)\n",
        "output_layer = Dense(1, activation='sigmoid')(inception_output)\n",
        "\n",
        "# Define the Inception model\n",
        "inception_model = Model(inputs=inception_input, outputs=output_layer)\n",
        "\n",
        "# Extract the output tensor of the Inception model\n",
        "inception_output = inception_model(inception_input)\n",
        "\n",
        "# Concatenate the outputs of the two models\n",
        "concatenated = concatenate([alx_output, inception_output])\n",
        "\n",
        "# Add a dense layer for further processing\n",
        "final_layer_1 = Dense(64, activation='relu')(concatenated)\n",
        "final_layer = Dense(16, activation='relu')(final_layer_1)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(final_layer)\n",
        "\n",
        "# Create the hybrid model\n",
        "model_hybrid = Model(inputs=[alx_input, inception_input], outputs=output_layer)\n",
        "\n",
        "# Compile the hybrid model\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "# Train the hybrid model\n",
        "model_hybrid.fit([X_train_scaled, X_train_scaled], y_train, epochs=20, batch_size=128, validation_split=0.2)\n",
        "\n",
        "\n",
        "y_prob_hybrid = model_hybrid.predict([X_test_scaled, X_test_scaled])\n",
        "y_pred = (y_prob_hybrid > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for the hybrid model\n",
        "end = time.time()\n",
        "\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time: {exe_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69xY2_OWCbtK"
      },
      "outputs": [],
      "source": [
        "! pip install shap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lv_BXTfxduI"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import tqdm\n",
        "\n",
        "# Define the function for the hybrid model prediction\n",
        "def hybrid_predict(sample_instances):\n",
        "    return model_hybrid.predict([sample_instances, sample_instances])\n",
        "\n",
        "# Create a sample of instances you want to explain\n",
        "sample_instances = X_train[:500]  # Change this to the number of instances you want\n",
        "sample_instance =X_test[:500]\n",
        "# Create a background summary using k-means clustering for the training data\n",
        "background_summary = shap.kmeans(X_train, 8)\n",
        "\n",
        "# Create SHAP explainer using KernelExplainer with summarized background data\n",
        "with tqdm.tqdm(total=len(sample_instances)) as pbar:\n",
        "    explainer = shap.KernelExplainer(hybrid_predict, background_summary)\n",
        "    pbar.update(len(sample_instances))\n",
        "\n",
        "# Calculate SHAP values for the sample instances\n",
        "with tqdm.tqdm(total=len(sample_instance)) as pbar:\n",
        "    shap_values = explainer.shap_values(sample_instance)\n",
        "    pbar.update(len(sample_instance))\n",
        "\n",
        "# Choose an instance from the sample to create a force plot for\n",
        "instance_index = 0  # Change this to the index of the instance you want\n",
        "instance_to_explain = sample_instances[instance_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuqA47gKpanQ"
      },
      "outputs": [],
      "source": [
        "feature_names = ['BMI', 'Smoking', 'AlcoholDrinking', 'Stroke', 'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory', 'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth', 'SleepTime', 'Asthma', 'KidneyDisease', 'SkinCancer']\n",
        "print(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G--LiqtKpa-_"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('summary Plot for Model Prediction')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "shap.summary_plot(shap_values, sample_instance, plot_type=\"bar\", feature_names=feature_names, show=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0iSBWXhKPyH"
      },
      "outputs": [],
      "source": [
        "background_summary = shap.kmeans(X_train, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwLsU-oIKQ6E"
      },
      "outputs": [],
      "source": [
        "feature_names = background_summary.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubltECFFK4hf"
      },
      "outputs": [],
      "source": [
        "cluster_centers = background_summary.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZxWHBqpKqLe"
      },
      "outputs": [],
      "source": [
        "!pip install shap --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of49qgeVKhuR"
      },
      "outputs": [],
      "source": [
        "feature_names = [str(i) for i in range(len(sample_instance[0]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpS0UREMKSBw"
      },
      "outputs": [],
      "source": [
        "feature_names = [str(i) for i in range(len(sample_instance[0]))]\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('summary Plot for Model Prediction')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "shap.summary_plot(shap_values, sample_instance, plot_type=\"bar\", feature_names=feature_names, show=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOBoYQY1KKwm"
      },
      "outputs": [],
      "source": [
        "feature_names = [str(i) for i in range(len(sample_instance[0]))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4W_Qlclxkxi"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IuUkTzjztA-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_prob_hybrid = model_hybrid.predict([X_test_scaled, X_test_scaled])\n",
        "y_pred = (y_prob_hybrid > 0.5).astype(int)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate percentages\n",
        "conf_matrix_percent = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix_percent, annot=True, cmap=\"Blues\", fmt=\".2f\", cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.title('Confusion Matrix (%)')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOeD3hVlyW3G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_prob_hybrid = model_hybrid.predict([X_test_scaled, X_test_scaled])\n",
        "y_pred = (y_prob_hybrid > 0.5).astype(int)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyeBjjWUTDRu"
      },
      "source": [
        "# 10-Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRm0gyx5TSVv"
      },
      "source": [
        "## ALEXNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqk6fnJKTSWM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# Define the AlexNet-like model\n",
        "# # Reshape the input data for 1D convolution\n",
        "# X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "# X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "alx_model = Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(8, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
        "alx_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "alx_model.fit(X_train_scaled, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "# Model evaluation on the test set\n",
        "y_pred_alx = alx_model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_alx > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "end = time.time()\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time:{exe_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywzX2epHTSWN"
      },
      "source": [
        "## Googlenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr2XFAOwTSWN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the towers (branches)\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(32, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(input_layer)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "# Concatenate the towers\n",
        "output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "output = Flatten()(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(4, activation='relu')(output)\n",
        "output = Dropout(0.5)(output)\n",
        "# output = Dense(16, activation='tanh')(output)\n",
        "\n",
        "# Output layer for binary classification with sigmoid activation\n",
        "output_layer = Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "# Build the model\n",
        "custom_inception_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model for binary classification\n",
        "custom_inception_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "custom_inception_model.fit(X_train_scaled, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_train_pred = (custom_inception_model.predict(X_train_scaled) > 0.5).astype(int)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = (custom_inception_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "# print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Googlenet Accuracy:\", test_accuracy)\n",
        "print(\"Googlenet Precision:\", precision)\n",
        "print(\"Googlenet Recall:\", recall)\n",
        "print(\"Googlenet F1 Score:\", f1)\n",
        "print(\"Googlenet Execution Time:\", execution_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIhyFBCrTSWO"
      },
      "source": [
        "## alexnet-googlnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7164XRHTSWP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the input layer for the AlexNet model\n",
        "alx_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "alx_model = tf.keras.Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(8, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Extract the output tensor of the AlexNet model\n",
        "alx_output = alx_model(alx_input)\n",
        "\n",
        "# Define the input layer for the Inception model\n",
        "inception_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(32, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(inception_input)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "inception_output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "inception_output = Flatten()(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(4, activation='relu')(inception_output)\n",
        "inception_output = Dropout(0.5)(inception_output)\n",
        "output_layer = Dense(1, activation='sigmoid')(inception_output)\n",
        "\n",
        "# Define the Inception model\n",
        "inception_model = Model(inputs=inception_input, outputs=output_layer)\n",
        "\n",
        "# Extract the output tensor of the Inception model\n",
        "inception_output = inception_model(inception_input)\n",
        "\n",
        "# Concatenate the outputs of the two models\n",
        "concatenated = concatenate([alx_output, inception_output])\n",
        "\n",
        "# Add a dense layer for further processing\n",
        "final_layer_1 = Dense(64, activation='tanh')(concatenated)\n",
        "final_layer = Dense(16, activation='tanh')(final_layer_1)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(final_layer)\n",
        "\n",
        "# Create the hybrid model\n",
        "model_hybrid = Model(inputs=[alx_input, inception_input], outputs=output_layer)\n",
        "\n",
        "# Compile the hybrid model\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "# Train the hybrid model\n",
        "model_hybrid.fit([X_train_scaled, X_train_scaled], y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_prob_hybrid = model_hybrid.predict([X_test_scaled, X_test_scaled])\n",
        "y_pred = (y_prob_hybrid > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for the hybrid model\n",
        "end = time.time()\n",
        "\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time: {exe_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpZli9Ngrly"
      },
      "source": [
        "# 30 Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdAuqGrjgxSg"
      },
      "source": [
        "## ALEXNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JC_Xfy-gxSh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "\n",
        "# Define the AlexNet-like model\n",
        "# # Reshape the input data for 1D convolution\n",
        "# X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "# X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "alx_model = Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(8, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
        "alx_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "alx_model.fit(X_train_scaled, y_train, epochs=30, batch_size=128, validation_split=0.2)\n",
        "# Model evaluation on the test set\n",
        "y_pred_alx = alx_model.predict(X_test_scaled)\n",
        "y_pred = (y_pred_alx > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "end = time.time()\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time:{exe_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-AU2CPFgxSk"
      },
      "source": [
        "## Googlenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_LlswpVgxSl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the input layer\n",
        "input_layer = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "\n",
        "# Define the towers (branches)\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_2 = Conv1D(32, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(input_layer)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(input_layer)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "# Concatenate the towers\n",
        "output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "output = Flatten()(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(16, activation='relu')(output)\n",
        "output = Dense(4, activation='relu')(output)\n",
        "output = Dropout(0.5)(output)\n",
        "# output = Dense(16, activation='tanh')(output)\n",
        "\n",
        "# Output layer for binary classification with sigmoid activation\n",
        "output_layer = Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "# Build the model\n",
        "custom_inception_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model for binary classification\n",
        "custom_inception_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "custom_inception_model.fit(X_train_scaled, y_train, epochs=30, batch_size=128, validation_split=0.2)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_train_pred = (custom_inception_model.predict(X_train_scaled) > 0.5).astype(int)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "y_test_pred = (custom_inception_model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "# print(\"Training Accuracy:\", train_accuracy)\n",
        "print(\"Googlenet Accuracy:\", test_accuracy)\n",
        "print(\"Googlenet Precision:\", precision)\n",
        "print(\"Googlenet Recall:\", recall)\n",
        "print(\"Googlenet F1 Score:\", f1)\n",
        "print(\"Googlenet Execution Time:\", execution_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9cz7lVQgxSn"
      },
      "source": [
        "## alexnet-googlnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0syc96jno8tb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define the input layer for the AlexNet model\n",
        "alx_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "alx_model = tf.keras.Sequential([\n",
        "    Conv1D(128, kernel_size=5, activation='tanh', input_shape=(X_train_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(8, kernel_size=2, padding='same', activation='tanh'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(16, activation='tanh'),\n",
        "    Dense(8, activation='tanh'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Extract the output tensor of the AlexNet model\n",
        "alx_output = alx_model(alx_input)\n",
        "\n",
        "# Define the input layer for the Inception model\n",
        "inception_input = Input(shape=(X_train_scaled.shape[1], 1))\n",
        "tower_1 = Conv1D(64, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(8, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_2 = Conv1D(32, 3, padding='same', activation='tanh')(tower_2)\n",
        "tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(inception_input)\n",
        "tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "tower_4 = MaxPooling1D(3, strides=1, padding='same')(inception_input)\n",
        "tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "inception_output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "inception_output = Flatten()(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(16, activation='relu')(inception_output)\n",
        "inception_output = Dense(4, activation='relu')(inception_output)\n",
        "inception_output = Dropout(0.5)(inception_output)\n",
        "output_layer = Dense(1, activation='sigmoid')(inception_output)\n",
        "\n",
        "# Define the Inception model\n",
        "inception_model = Model(inputs=inception_input, outputs=output_layer)\n",
        "\n",
        "# Extract the output tensor of the Inception model\n",
        "inception_output = inception_model(inception_input)\n",
        "\n",
        "# Concatenate the outputs of the two models\n",
        "concatenated = concatenate([alx_output, inception_output])\n",
        "\n",
        "# Add a dense layer for further processing\n",
        "final_layer_1 = Dense(64, activation='tanh')(concatenated)\n",
        "final_layer = Dense(16, activation='tanh')(final_layer_1)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(1, activation='sigmoid')(final_layer)\n",
        "\n",
        "# Create the hybrid model\n",
        "model_hybrid = Model(inputs=[alx_input, inception_input], outputs=output_layer)\n",
        "\n",
        "# Compile the hybrid model\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "# Train the hybrid model\n",
        "model_hybrid.fit([X_train_scaled, X_train_scaled], y_train, epochs=30, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_prob_hybrid = model_hybrid.predict([X_test_scaled, X_test_scaled])\n",
        "y_pred = (y_prob_hybrid > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for the hybrid model\n",
        "end = time.time()\n",
        "\n",
        "exe_time = end - start\n",
        "res_accuracy = accuracy_score(y_test, y_pred)\n",
        "res_precision = precision_score(y_test, y_pred)\n",
        "res_recall = recall_score(y_test, y_pred)\n",
        "res_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {res_accuracy}\")\n",
        "print(f\"F1-score: {res_f1}\")\n",
        "print(f\"Precision: {res_precision}\")\n",
        "print(f\"Recall: {res_recall}\")\n",
        "print(f\"Execution Time: {exe_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhU8_GpVYwee"
      },
      "source": [
        "## K-FCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE9bsRY2cz1v"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming X_train_scaled, X_test_scaled, y_train, and y_test are already defined\n",
        "\n",
        "# Reshape the input data to add a channel dimension\n",
        "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Define the hybrid model architecture\n",
        "def build_hybrid_model(input_shape):\n",
        "    # Define the input layers for the two sub-models\n",
        "    alx_input = Input(shape=input_shape)\n",
        "    inception_input = Input(shape=input_shape)\n",
        "\n",
        "    # Define the architecture for the AlexNet model\n",
        "    alx_model = tf.keras.Sequential([\n",
        "        Conv1D(128, kernel_size=5, activation='tanh', input_shape=input_shape),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(4, kernel_size=2, padding='same', activation='tanh'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(32, activation='tanh'),\n",
        "        Dense(16, activation='tanh'),\n",
        "        Dense(16, activation='tanh'),\n",
        "        Dense(4, activation='tanh'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Define the architecture for the Inception model\n",
        "    tower_1 = Conv1D(128, 1, padding='same', activation='relu')(inception_input)\n",
        "    tower_2 = Conv1D(64, 1, padding='same', activation='tanh')(inception_input)\n",
        "    tower_2 = Conv1D(64, 3, padding='same', activation='tanh')(tower_2)\n",
        "    tower_3 = Conv1D(16, 1, padding='same', activation='tanh')(inception_input)\n",
        "    tower_3 = Conv1D(16, 5, padding='same', activation='tanh')(tower_3)\n",
        "    tower_4 = MaxPooling1D(3, strides=1, padding='same')(inception_input)\n",
        "    tower_4 = Conv1D(8, 1, padding='same', activation='tanh')(tower_4)\n",
        "\n",
        "    inception_output = concatenate([tower_1, tower_2, tower_3, tower_4], axis=-1)\n",
        "    inception_output = Flatten()(inception_output)\n",
        "    inception_output = Dense(16, activation='tanh')(inception_output)\n",
        "    inception_output = Dense(16, activation='tanh')(inception_output)\n",
        "    inception_output = Dense(4, activation='tanh')(inception_output)\n",
        "    inception_output = Dropout(0.5)(inception_output)\n",
        "    output_layer = Dense(1, activation='sigmoid')(inception_output)\n",
        "\n",
        "    # Define the hybrid model\n",
        "    concatenated = concatenate([alx_model(alx_input), output_layer])\n",
        "    final_layer_1 = Dense(32, activation='tanh')(concatenated)\n",
        "    final_layer = Dense(16, activation='tanh')(final_layer_1)\n",
        "    output_layer = Dense(1, activation='sigmoid')(final_layer)\n",
        "    model_hybrid = Model(inputs=[alx_input, inception_input], outputs=output_layer)\n",
        "\n",
        "    return model_hybrid\n",
        "\n",
        "# Define K-fold cross-validation\n",
        "k_folds = 10\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Define lists to store evaluation metrics for each fold\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Perform K-fold cross-validation\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train_scaled)):\n",
        "    print(f'Fold {fold+1}/{k_folds}')\n",
        "\n",
        "    # Get the data for this fold\n",
        "    X_train_fold = [X_train_scaled[train_indices], X_train_scaled[train_indices]]\n",
        "    y_train_fold = y_train[train_indices]\n",
        "    X_val_fold = [X_train_scaled[val_indices], X_train_scaled[val_indices]]\n",
        "    y_val_fold = y_train[val_indices]\n",
        "\n",
        "    # Build the hybrid model\n",
        "    model_hybrid = build_hybrid_model(input_shape=X_train_scaled.shape[1:])\n",
        "\n",
        "    # Compile the model\n",
        "    model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model_hybrid.fit(X_train_fold, y_train_fold, epochs=30, batch_size=128, validation_data=(X_val_fold, y_val_fold))\n",
        "\n",
        "    # Evaluate the model on the validation data\n",
        "    y_pred = (model_hybrid.predict(X_val_fold) > 0.5).astype(int)\n",
        "\n",
        "    # Calculate evaluation metrics for this fold\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
        "    precision = precision_score(y_val_fold, y_pred)\n",
        "    recall = recall_score(y_val_fold, y_pred)\n",
        "    f1 = f1_score(y_val_fold, y_pred)\n",
        "\n",
        "    # Store the evaluation metrics for this fold\n",
        "    accuracy_scores.append(accuracy)\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1-score: {f1}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# Calculate average evaluation metrics across all folds\n",
        "avg_accuracy = np.mean(accuracy_scores)\n",
        "avg_precision = np.mean(precision_scores)\n",
        "avg_recall = np.mean(recall_scores)\n",
        "avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "end = time.time()\n",
        "exe_time = end - start\n",
        "\n",
        "# Print average evaluation metrics\n",
        "print(\"\\nAverage Evaluation Metrics Across All Folds:\")\n",
        "print(f\"Average Accuracy: {avg_accuracy}\")\n",
        "print(f\"Average F1-score: {avg_f1}\")\n",
        "print(f\"Average Precision: {avg_precision}\")\n",
        "print(f\"Average Recall: {avg_recall}\")\n",
        "print(f\"Total Execution Time: {exe_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXgwtSlDmCbY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO9cCDvnmC-G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the variables\n",
        "models = ['AlexNet', 'GoogleNet', 'Galex-Heart']\n",
        "accuracy = [87.29, 86.93, 90.00]\n",
        "precision = [83.76, 91.55, 89.56]\n",
        "recall = [92.63, 81.50, 90.66]\n",
        "f1_score = [87.10, 86.23, 90.11]\n",
        "\n",
        "# Set width of bar\n",
        "barWidth = 0.15\n",
        "\n",
        "# Set position of bar on X axis with added distance\n",
        "r1 = np.arange(len(models)) * 2\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "r4 = [x + barWidth for x in r3]\n",
        "\n",
        "# Define custom CSS colors\n",
        "css_colors = [(0.71, 0.2, 0.4), (0.4, 0.6, 0.2), (0.1, 0.5, 0.8)]  # Custom CSS colors\n",
        "\n",
        "# Make the plot\n",
        "plt.figure(facecolor='white')  # Set background color to white\n",
        "bars1 = plt.bar(r1, accuracy, color=css_colors[0], width=barWidth, edgecolor='white', label='Accuracy')\n",
        "bars2 = plt.bar(r2, precision, color=css_colors[1], width=barWidth, edgecolor='white', label='Precision')\n",
        "bars3 = plt.bar(r3, recall, color=css_colors[2], width=barWidth, edgecolor='white', label='Recall')\n",
        "bars4 = plt.bar(r4, f1_score, color='orange', width=barWidth, edgecolor='white', label='F1-score')\n",
        "\n",
        "# Add xticks on the middle of the group bars\n",
        "# plt.xlabel('Models', position=(0.5, 1.08), fontweight='normal')  # Simplified x-axis label with normal font weight\n",
        "plt.xticks([r + 1.5 * barWidth for r in r2], models, rotation=0, ha='center')\n",
        "\n",
        "# Label y axis with 'Scores'\n",
        "plt.ylabel('Scores', position=(0, 0.5))\n",
        "\n",
        "# Create legend with box in the bottom right corner\n",
        "plt.legend(loc='lower right', bbox_to_anchor=(1, 0), title='Metrics')\n",
        "\n",
        "# Hide the top and right spines\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Add model label on top of the graph\n",
        "plt.text(2.5, 100, '30 epochs', ha='center', fontsize=12)\n",
        "\n",
        "# Show graphic\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz9-evM_t-AC"
      },
      "source": [
        "# AUC-ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NcEvloYyb7A"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve and AUC for AlexNet-like model\n",
        "fpr_alx, tpr_alx, thresholds_alx = roc_curve(y_test, y_pred_alx)\n",
        "roc_auc_alx = auc(fpr_alx, tpr_alx)\n",
        "\n",
        "# Calculate ROC curve and AUC for Inception model\n",
        "fpr_inception, tpr_inception, thresholds_inception = roc_curve(y_test, y_test_pred)\n",
        "roc_auc_inception = auc(fpr_inception, tpr_inception)\n",
        "\n",
        "# Calculate ROC curve and AUC for Hybrid model\n",
        "fpr_hybrid, tpr_hybrid, thresholds_hybrid = roc_curve(y_test, y_prob_hybrid)\n",
        "roc_auc_hybrid = auc(fpr_hybrid, tpr_hybrid)\n",
        "\n",
        "# Plot all ROC curves in one graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_alx, tpr_alx, color='blue', lw=2, label='AlexNet ROC curve (AUC = %0.2f)' % roc_auc_alx)\n",
        "plt.plot(fpr_inception, tpr_inception, color='green', lw=2, label='GoogleNet ROC curve (AUC = %0.2f)' % roc_auc_inception)\n",
        "plt.plot(fpr_hybrid, tpr_hybrid, color='red', lw=2, label='Galex-Heart ROC curve (AUC = %0.2f)' % roc_auc_hybrid)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CyIJXQEu16B"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve and AUC for AlexNet-like model\n",
        "fpr_alx, tpr_alx, thresholds_alx = roc_curve(y_test, y_pred_alx)\n",
        "roc_auc_alx = auc(fpr_alx, tpr_alx)\n",
        "\n",
        "# Calculate ROC curve and AUC for Inception model\n",
        "fpr_inception, tpr_inception, thresholds_inception = roc_curve(y_test, y_test_pred)\n",
        "roc_auc_inception = auc(fpr_inception, tpr_inception)\n",
        "\n",
        "# Calculate ROC curve and AUC for Hybrid model\n",
        "fpr_hybrid, tpr_hybrid, thresholds_hybrid = roc_curve(y_test, y_prob_hybrid)\n",
        "roc_auc_hybrid = auc(fpr_hybrid, tpr_hybrid)\n",
        "\n",
        "# Plot all ROC curves in one graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_alx, tpr_alx, color='blue', lw=2, label='AlexNet ROC curve (AUC = %0.2f)' % roc_auc_alx)\n",
        "plt.plot(fpr_inception, tpr_inception, color='green', lw=2, label='GoogleNet ROC curve (AUC = %0.2f)' % roc_auc_inception)\n",
        "plt.plot(fpr_hybrid, tpr_hybrid, color='red', lw=2, label='Galex-Heart ROC curve (AUC = %0.2f)' % roc_auc_hybrid)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBIjant-0oRe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate ROC curve and AUC for AlexNet-like model\n",
        "fpr_alx, tpr_alx, thresholds_alx = roc_curve(y_test, y_pred_alx)\n",
        "roc_auc_alx = auc(fpr_alx, tpr_alx)\n",
        "\n",
        "# Calculate ROC curve and AUC for Inception model\n",
        "fpr_inception, tpr_inception, thresholds_inception = roc_curve(y_test, y_test_pred)\n",
        "roc_auc_inception = auc(fpr_inception, tpr_inception)\n",
        "\n",
        "# Calculate ROC curve and AUC for Hybrid model\n",
        "fpr_hybrid, tpr_hybrid, thresholds_hybrid = roc_curve(y_test, y_prob_hybrid)\n",
        "roc_auc_hybrid = auc(fpr_hybrid, tpr_hybrid)\n",
        "\n",
        "# Plot all ROC curves in one graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_alx, tpr_alx, color='blue', lw=2, label='AlexNet ROC curve (AUC = %0.2f)' % roc_auc_alx)\n",
        "plt.plot(fpr_inception, tpr_inception, color='green', lw=2, label='Inception ROC curve (AUC = %0.2f)' % roc_auc_inception)\n",
        "plt.plot(fpr_hybrid, tpr_hybrid, color='red', lw=2, label='Hybrid Model ROC curve (AUC = %0.2f)' % roc_auc_hybrid)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dziDAlUU0Nii"
      },
      "source": [
        "# Bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtXrdP7FNCWs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the variables\n",
        "models = ['AlexNet', 'GoogleNet', 'Galex-Heart']\n",
        "accuracy = [86.47, 85.47, 89.46]\n",
        "precision = [87.74, 93.81, 88.53]\n",
        "recall = [84.99, 77.16, 90.79]\n",
        "f1_score = [86.34, 84.68, 89.65]\n",
        "\n",
        "# Set width of bar\n",
        "barWidth = 0.15\n",
        "\n",
        "# Set position of bar on X axis with added distance\n",
        "r1 = np.arange(len(models)) * 2\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        "r4 = [x + barWidth for x in r3]\n",
        "\n",
        "# Define custom CSS colors\n",
        "css_colors = [(0.71, 0.2, 0.4), (0.4, 0.6, 0.2), (0.1, 0.5, 0.8)]  # Custom CSS colors\n",
        "\n",
        "# Make the plot\n",
        "plt.figure(facecolor='white')  # Set background color to white\n",
        "bars1 = plt.bar(r1, accuracy, color=css_colors[0], width=barWidth, edgecolor='white', label='Accuracy')\n",
        "bars2 = plt.bar(r2, precision, color=css_colors[1], width=barWidth, edgecolor='white', label='Precision')\n",
        "bars3 = plt.bar(r3, recall, color=css_colors[2], width=barWidth, edgecolor='white', label='Recall')\n",
        "bars4 = plt.bar(r4, f1_score, color='orange', width=barWidth, edgecolor='white', label='F1-score')\n",
        "\n",
        "# Add xticks on the middle of the group bars\n",
        "# plt.xlabel('Models', position=(0.5, 1.08), fontweight='normal')  # Simplified x-axis label with normal font weight\n",
        "plt.xticks([r + 1.5 * barWidth for r in r2], models, rotation=0, ha='center')\n",
        "\n",
        "# Label y axis with 'Scores'\n",
        "plt.ylabel('Scores', position=(0, 0.5))\n",
        "\n",
        "# Create legend with box in the bottom right corner\n",
        "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
        "\n",
        "# Hide the top and right spines\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Add model label on top of the graph\n",
        "plt.text(2.5, 100, '20 epochs', ha='center', fontsize=12)\n",
        "\n",
        "# Show graphic\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mmXs9u20LvH9"
      },
      "outputs": [],
      "source": [
        "# Activation functions and their corresponding accuracies\n",
        "activation_functions = ['ReLu', 'Tanh', 'Linear']\n",
        "activation_ablation_accuracies = [0.88, 0.89, 0.85]  # Example accuracies\n",
        "\n",
        "# Plotting the bar graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(activation_functions, activation_ablation_accuracies, color=['blue', 'blue', 'blue'])\n",
        "plt.xlabel('Activation Functions')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy of Different Activation Functions')\n",
        "plt.ylim(0, 1)  # Assuming accuracy is between 0 and 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHc1boaGMP92"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1eKY96iy29aa3DZ6NYUFtKqgV-6zIl5Bt",
      "authorship_tag": "ABX9TyOFxIDgCT117P6GO8O+PHYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}